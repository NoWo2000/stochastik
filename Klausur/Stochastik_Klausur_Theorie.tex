\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
%\usepackage{float}
\usepackage{algorithm}
\usepackage{arevmath}     % For math symbols
\usepackage[noend]{algpseudocode}

\begin{document}
\section{Theorie}
\setlength{\parindent}{0em}

\subsection{Wahrscheinlichkeitsraum}

\subsubsection{Definition}
Ein Wahrscheinlichkeitsraum ist ein Tripel $(\Omega, \mathcal{A}, P)$ bestehend aus der Grundmenge $\Omega$, einer $\sigma$-Algebra $\mathcal{A} \subseteq  2^{\Omega}$ und einer Abbildung
$P : \mathcal{A} \to [0,1]$
\begin{align*}
(i) & \; P(\Omega) = 1 \\
(ii) & \;  P \biggl(  \bigcup_i A_i  \biggr) = \sum_i P(A_i), \text{ mit } A_i \cap A_j = \emptyset \text{ für } i \neq j
\end{align*}
Die Elemente von $\Omega$ werden elementare Ereignisse und die von $\mathcal{A}$ Ereignisse genannt. Mengen M mit $P(M) = 0$ werden Nullmengen genannt.
Die Abbildung $P$ wird Wahrscheinlichkeitsmaß genannt.

\subsubsection{$\sigma$-Algebra}
Es sei $\Omega$ eine Menge und $\mathcal{A} \subset  2^{\Omega}$ ein System von Teilmengen (= Ereignissen). $\mathcal{A}$ heißt $\sigma$-Algebra (Sigma-Algebra) falls gilt:
\begin{align*}
& (i) \; \Omega \in \mathcal{A} \\
& (ii) \; A \in \mathcal{A} \Rightarrow A^c \in \mathcal{A} \\
& (iii) \; A_i \in \mathcal{A} \Rightarrow \bigcup_i A_i \in \mathcal{A} 
\end{align*}
$(A^c = \Omega \setminus A)$

\subsubsection{Diskreter Wahrscheinlichkeitsraum}
Ein diskreter Wahrscheinlichkeitsraum ist ein Wahrscheinlichkeitsraum $(\Omega, \mathcal{A}, P)$, bei dem die Grundmenge $\Omega$ abzählbar ist und die Menge der Ereignisse $\mathcal{A} := 2^{\Omega}$ der Potenzmenge entspricht.

\subsubsection{Laplace Experiment}
Ein Laplace-Experiment ist ein Zufallsexperiment bei dem der Ereignisraum $\Omega$ endlich viele Elemente und ein Ereignis $A \subseteq \Omega$ die Wahrscheinlichkeit $P(A) = \frac{\#A}{\#\Omega}$ hat.


\subsection{Bedingte Wahrscheinlichkeit}
Für $A,B \in \mathcal{A}$ und $P(B) > 0$ heißt
\begin{align*}
& P(A \; | \;  B) = \frac{P(A \cap B)}{P(B)} \\
\end{align*}
die bedingte Wahrscheinlichkeit (von $A$ unter $B$).


\subsection{Variation und Kombination}
\begin{itemize}
\item $\# Var_k^n(\Omega, m.W.)  = n^k$
\item $\# Var_k^n(\Omega, o.W.)  = n_k = \frac{n!}{(n-k)!}$  
\item $\#Kom_k^n(\Omega, o.W.) = \binom{n}{k} = \frac{n!}{k! (n-k)!}$  
\item $\#Kom_k^n(\Omega, m.W.)  = \binom{n + k -1}{k}$  
\end{itemize}


\subsection{Spamfilter / Satz von Bayes}

\subsubsection{Satz der totalen Wahrscheinlichkeit}
Für eine Zerlegung  $\Omega = \bigcup_{j=1}^{n} B_j, \text{ mit } B_i \cap B_k = \emptyset \text{ für } i \neq k $, gilt
\begin{align*}
& P(A ) = \sum_{j=1}^{n}  P(A \; | \;  B_j) \cdot P(B_j)
\end{align*}

\subsubsection{Satz von Bayes}
Für $A,B \in \mathcal{A}$ mit  $P(B) > 0$ gilt
\begin{align*}
& P(A \; | \;  B) = \frac{P(B \; | \; A) \cdot P(A)} {P(B)} \\
\end{align*}

\subsubsection{Stochastische Unabhängigkeit}
Zwei Ereignisse $A,B$ heißen stochastisch unabhängig, falls
\begin{align*}
P(A \cap B) = P(A) \cdot P(B)
\end{align*}
gilt.  Gleichbedeutend damit ist  $P(A | B) = P(A)$ und $P(B  | A) = P(B)$.
\newpage
\subsubsection{Naiver Bayes'scher Spam Filter}
Gegeben ist eine E-Mail $E$.  Wir möchten anhand des Vorkommens bestimmter Wörter $A_1, \ldots A_n$ in der Mail entscheiden, ob es sich um eine erwünschte Mail $H$ oder eine unerwünschte Mail $S$ handelt.\\\\
Aus einer Datenbank kann man das Vorkommen dieser Wörter in allen E-Mails zählen und damit empirisch die Wahrscheinlichkeiten $P(A_i | S)$ und $P(A_i | H) $ des Vorkommens dieser Wörter in Spam und Ham Mails ermitteln.  Wir gehen davon aus, dass es sich bei der Mail  prinzipiell mit  Wahrscheinlichkeit $P(E= S) = P(E= H)= \frac{1}{2}$  um eine erwünschte  Mail $H$ oder eine unerwünschte Mail $S$  handeln kann. \\\\
 Wir machen zudem die (naive) Annahme, dass das Vorkommen der Wörter  stochastisch unabhängig ist, also 
\begin{align*}
P(A_1 \cap \cdots \cap A_n | S) = P(A_1 | S) \cdot P(A_2 | S) \cdots P(A_n | S) \\
P(A_1 \cap \cdots \cap A_n | H) = P(A_1 | H) \cdot P(A_2 | H) \cdots P(A_n | H)
\end{align*}
gilt.\\\\
Mit der Formel von Bayes und der totalen Wahrscheinlichkeit  können wir somit berechnen:
\begin{align*}
& P(E=S |  A_1 \cap \cdots \cap A_n) \quad \texttt{(-> "E =" wird im folgenden weggelassen)}\\[5pt]
&= \frac{P(A_1 \cap \cdots \cap A_n | S) \cdot P(S)}{P(A_1 \cap \cdots \cap A_n)} \quad \texttt{(-> Satz von Bayes)}\\[5pt]
&=  \frac{P(A_1 | S) \cdots P(A_n | S) \cdot P(S)}{P(A_1 \cap \cdots \cap A_n | H) \cdot P(H) + P(A_1 \cap \cdots \cap A_n | S) \cdot P(S)} \quad \frac{\texttt{(-> Stoch. Unabhängigkeit)}}{\texttt{(-> Satz der t. Wahrscheinlichkeit)}} \\[5pt]
&=  \frac{P(A_1 | S) \cdots P(A_n | S)}{P(A_1 \cap \cdots \cap A_n | H) + P(A_1 \cap \cdots \cap A_n | S)} \quad \texttt{(-> kürzen da P(H) = P(S))} \\[5pt]
&=  \frac{P(A_1 | S) \cdots P(A_n | S)}{P(A_1 | H) \cdots P(A_n | H)  + P(A_1 | S) \cdots P(A_n | S) } \quad \frac{•}{\texttt{(-> Stoch. Unabhängigkeit)}} \\
\end{align*}
Bemerkung: $P(E=H |  A_1 \cap \cdots \cap A_n) = 1- P(E=S |  A_1 \cap \cdots \cap A_n) $\\\\
\textbf{Schlussfolgerung:}\\
Die Wahrscheinlichkeiten $P(A_i | S)$ und $P(A_i | H) $ sind durch Empirie (Datenbank) bekannt. Daher lässt sich so nun die Wahrscheinlichkeit berechnen, ob es sich bei einer E-Mail, welche die Wörter $A_i$ enthält, um eine Spam E-Mail handelt.


\subsection{Zufallsvariablen}

\subsubsection{Allgemeine Zufallsvariable}
Sei $(\Omega, \mathcal{A}, P)$ ein Wahrscheinlichkeitsraum und $(\Omega', \mathcal{A}')$ ein Messraum. Eine Zufallsvariable ist eine Abbildung
$$X : \Omega \to \Omega'$$ 
so dass für alle Ereignisse $A' \in  \mathcal{A}'$
$$ X^{-1} (A') \in \mathcal{A}$$
 ein Ereignis in $\mathcal{A}$ ist. Urbilder von Ereignissen sind also Ereignisse.

\subsubsection{Messraum}
Ein Messraum ist ein Paar $(\Omega, \mathcal{A})$ bestehend aus einer Menge $\Omega$ und einer Sigma-Algebra $\mathcal{A} \subset 2^{\Omega}$.

\subsubsection{Reelle Zufallsvariable}
Unter einer reellen Zufallsvariable verstehen wir eine Zufallsvariable 
\begin{align*}
& X : \Omega \to \mathbb{R}^n \\
& X(\omega) := \biggl(X_1(\omega), \cdots , X_n(\omega)  \biggr) \; ,
\end{align*}
wobei $(\Omega, \mathcal{A}, P)$ ein Wahrscheinlichkeitsraum und das Tupel $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ der Messraum mit $\mathbb{R}^n$ und der Borel'schen Sigma-Algebra ist.

\pagebreak
\subsection{Erwartungswert}

\subsubsection{Definition}
Für eine reelle, integrierbare Zufallsvariable $X$ ist der Erwartungswert definiert durch
$$ \mathbb{E} (X) := \int_{\Omega} X \; dP \; .$$
Ist $(\Omega, \mathcal{A}, P)$ ein diskreter Wahrscheinlichkeitsraum und $X :\Omega \to \mathbb{R}$ eine eindimensionale reelle Zufallsvariable, so ist
$$ \mathbb{E} (X) = \sum_{\omega \in \Omega}  X(\omega) \cdot P(\omega)$$

\subsubsection{Eigenschaften}
Sind $X,Y : \Omega \to \mathbb{R}^n$   reelle, integrierbare  Zufallsvariablen und $a,b \in \mathbb{R}$ konstant, so gilt:
\begin{align*}
& \mathbb{E}(a \cdot X \pm b \cdot Y) = a \cdot \mathbb{E}(X) \pm b \cdot \mathbb{E}(Y) \\
& \forall x \in \Omega: X(x) \leq Y(x) \;   \Rightarrow \mathbb{E}(X) \leq \mathbb{E}(Y) \\
& X ,Y \text{ stoch. unabhängig} \Rightarrow   \mathbb{E}(X \cdot Y) =  \mathbb{E}(X) \cdot  \mathbb{E}(Y) \\
& \mathbb{E} (1_A) = P (A)
\end{align*}

\subsection{Varianz}
Für eine reelle Zufallsvariable $X$ ist die Varianz definiert durch
$$ \mathbb{V} (X) \; := \;  \mathbb{E}( (X - \mathbb{E}(X))^2) \;  = \; \mathbb{E}(X^2) -  \mathbb{E}(X)^2$$

\pagebreak
\subsection{Verteilungen}

\subsubsection{Normalverteilung}
Die Normalverteilung $N{(\mu,\sigma^2)}$ auf $\mathbb{R}$ ist definiert durch
\begin{align*}
& \text{Dichte: } f (x) : = \frac 1{\sigma \sqrt{2\pi}}e^{- \frac {1}{2} (\frac{x- \mu}{ \sigma})^2} \\
&  \text{Verteilung: } F(x) = N{(\mu,\sigma^2)}(-\infty , x) =  \int_{-\infty}^{x}  \frac 1{\sigma \sqrt{2\pi}}e^{- \frac {1}{2} (\frac{t- \mu}{ \sigma})^2}dt\\
\end{align*}
\textbf{Erwartungswert und Varianz bei $X \sim N(\mu, \sigma^2)$:}
\begin{align*}
& \mathbb{E}(X) = \mu \\
& \mathbb{V}(X) = \sigma^2
\end{align*}

\subsubsection{Verteilungsfunktion}
Für eine reelle Zufallsvariable $X$ heißt 
\begin{align*} 
& F_X : \Omega \to [0,1] \\
& F_X (x) := P (X \leq x) := P_X (( -\infty, x )) = P(X^{-1} (-\infty, x))
\end{align*}
Verteilungsfunktion von $X$.

\subsubsection{Gleichverteilung}
Die Gleichverteilung $U{(a,b)}$ auf einem Intervall $(a,b) \subset \mathbb{R}$ ist definiert durch
\begin{align*}
& \text{Dichte: } f (x) : = \frac{1_{(a,b)}}{|b-a| } \\
& \text{Verteilung: } F (x) =  P_f( (-\infty, x))  =  \int_{-\infty}^{x} \frac{1_{(a,b)}}{|b-a|} dt\\\
& = \begin {cases} 0 \text{ für } x \leq a \\   \frac{x-a}{|b-a|} \text{ für } a \leq x \leq b \\ 1 \text{ für }  x \geq b \\  \end{cases}
\end{align*}
\textbf{Erwartungswert und Varianz bei $X \sim U(a,b)$:}
\begin{align*}
& \mathbb{E}(X) = \frac{a+b}2 \\
& \mathbb{V}(X) = \mathbb{E}(X^2) - \left({\mathbb{E}(X)} \right)^2  = \frac{1}{3}\frac{b^3  - a^3}{b - a} - \left( {\frac{a + b}{2}} \right)^2 \\
    &= \frac{1}{12}(b - a)^2
\end{align*}

\subsubsection{Dichte}
Sei $\Omega \subset \mathbb{R}^n$ und $(\Omega, \mathcal{A})$ ein Messraum, wobei alle $A \in \mathcal{A}$ Lebesgue-messbar sind.
 Eine Funktion $f: \Omega \to \mathbb{R}$ heißt Dichte, falls für ihr Lebesgue-Integral $\int_{\Omega} f d \mu = 1$ gilt.


\subsection{Schwaches Gesetz der großen Zahlen}

\subsubsection{Definition}
Seien $X_i : \Omega \to \mathbb{R}$ unabhängige, reelle Zufallsvariablen mit $\mathbb{E}(X_i) = \mu < \infty$ und $\mathbb{V}(X_i) = \sigma < \infty$, dann gilt
\begin{align*}
P \bigl  ( \bigl | \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \bigr |  \geq \epsilon \bigr) \leq \frac{\sigma}{ n \cdot \epsilon^2} \; \; \underset{n \to \infty}{\longrightarrow} 0
\end{align*}
(stochastische Konvergenz). 

\subsubsection{Bedeutung}
Das schwache Gesetz der großen Zahlen besagt, dass das arithmetische Mittel einer großen Stichprobe einer Zufallsvariable mit einer beliebig kleinen Wahrscheinlichkeit dem Erwartungswert der Zufallsvariable entspricht.\\

\textbf{Gegenteilige (äquivalente) Formulierung:}\\
Die Wahrscheinlichkeit, dass die Differenz zwischen beobachteter relativer Häufigkeit und theoretischer Wahrscheinlichkeit kleiner ist als eine beliebig kleine positive Zahl $\epsilon$, geht für eine unendlich große Stichprobe gegen 1.


\subsection{Zentraler Grenzwertsatz}

\subsubsection{Definition}
Sei $(\Omega, \mathcal{A}, P)$ ein Wahrscheinlichkeitsraum und $X_n :  \Omega \to \mathbb{R}$  eine folge stochastisch unabhängiger, identisch verteilter, reeller Zufallsvariablen mit $\mathbb{E}(X_n) = \mu$ und $\mathbb{V}(X_n)= \sigma^2$. Dann gilt für das arithmetische Mittel $S_n:= \frac{1}{n} \sum_{i=1}^n X_i$
\begin{align*}
P_{ \frac{\sqrt{n}}{\sigma} (S_n-\mu)} \to P_{N(0,1)}
\end{align*}
wobei $ P_{N(0,1)}$ das Wahrscheinlichkeits-Maß mit der Dichte $ \frac {1}{ \sqrt{2\pi}}e^{- \frac {1}{2} x^2}$ ist.
\subsubsection{Bedeutung}
Die Summe von $n$ identisch verteilten, stochastisch unabhängigen Zufallsvariablen ist näherungsweise normalverteilt.\\

\textbf{Beispiel Würfel:}\\
Die Augensumme von $n \to \infty$ Würfeln ist normalverteilt, wenn alle Würfel von einander stochastisch unabhängig und gleichverteilt sind.

\pagebreak
\subsection{Schätzer}

\subsubsection{Ausgangslage}
Angenommen man findet einen Apparat, der zufällig Zahlen in einem Intervall $[0, \rho]$ ausgibt. Anhand von Beobachtungen der Zahlen möchte man $\rho$ schätzen. Wir machen die Annahme, dass alle Zahlen in dem Intervall gleich wahrscheinlich auftreten und  nehmen $n$ Stichproben $X_1, \cdots, X_n$. Einen Schätzer für $\rho$ bezeichnen wir mit $T_n$.

\subsubsection{Maximalwert-Schätzer}
Eine einfache und einleuchtende Idee ist es, $\rho$ durch die größte beobachtete Zahl zu schätzen, also
$T_n^{max} := \max(X_1, \cdots, X_n)$.\\
Dieser Schätzer konvergiert für $n \to \infty$ gegen $\rho$, also $$P(| T_n^{max} - \rho | \geq \epsilon) \underset{n \to \infty}{\longrightarrow} 0$$\\
Der Erwartungswert dieses Schätzers ist $$ \mathbb{E}(T_n^{max} ) = \frac{n}{n+1} \rho    \underset{n \to \infty}{\longrightarrow} \rho$$


\subsubsection{Erwartungswert-Schätzer}
Da das Auftreten der Zahlen gleich wahrscheinlich ist, ist der Erwartungswert des Zufallsexperiments $\rho /2$. Unter Berufung auf das  schwache Gesetz der großen Zahlen erscheint der Schätzer
$T_n^{E} :=  2 \cdot \biggl( \frac{1}{n} \sum_{i=1}^n X_i \biggr)$ sehr plausibel.\\
Dieser Schätzer konvergiert für $n \to \infty$ gegen $\rho$, also $$P(| T_n^{E} - \rho | \geq \epsilon) \underset{n \to \infty}{\longrightarrow} 0$$\\
Der Erwartungswert dieses Schätzers ist $$ \mathbb{E}(T_n^{E} ) = \rho$$

\subsubsection{Bedeutung}
Beide Schätzer sind asymptotisch exakt und konvergieren gegen $\rho$ (= Erwartungstreu). Der Erwartungswert-Schätzer ist aber durchschnittlich (Erwartungswert) früher exakt $\rho$ als der Maximalwert-Schätzer mit seinem Bias $\frac{n}{n+1}$. Die Varianz von beiden Schätzern geht für $n \to \infty$ gegen 0, beide werden also immer aussagekräftiger mit zunehmender Stichprobenanzahl.\\
Kein Schätzer ist allgemein perfekt, den aktuell passenden Schätzer kann man nach Kriterien wie Konvergenzgeschwindigkeit zum gesuchten Wert, Konvergenzgeschwindigkeit des Erwartungswertes und der Konvergenzgeschwindigkeit der Varianz auswählen.

\pagebreak
\subsection{Informationstheorie / Entropie}

\subsubsection{Ausgangslage}
Gegeben eine reelle, diskrete Zufallsvariable $X :\mathcal{X} \to \mathbb{R}$ mit endlichem Grundraum $\#\mathcal{X}  \geq 2$ und Verteilung $Q$.\\
Mit $L : \mathcal{X} \to \mathbb{N}$ bezeichnen wir die Anzahl an Fragen $L(x)$, die benötigt werden, um bei einer gewählten Strategie den Wert von $X = x$ zu erraten.\\
Wir suchen eine Strategie, so dass die mittlere Anzahl an Fragen  (Erwartungswert) $\mathbb{E}(L(X)) := \sum_{x \in \mathcal{X}} L(x) Q(x)$  möglichst klein ist.

\subsubsection{Wörter und Wortmenge}
Gegeben sei eine endliche Menge $\mathcal{A}$ mit $\# A \geq 2$, genannt Alphabet.
Ein Wort der Länge $k$ ist gegeben durch ein Tupel $w = b_1b_2 \cdots b_k$ mit Buchstaben $b_i \in \mathcal {A}$.\\
Die Menge aller Wörter bezeichnen wir mit 
$$\mathcal{W} (\mathcal{A}) := \{  b_1 \cdots b_k \; | \; i \in \mathbb{N}, \; b_i \in \mathcal{A} \} $$
 Mit $l(w):= k$ für $w=b_1 \cdots b_k  $ bezeichnen wir die Länge des Wortes.
 
\subsubsection{Kode und Präfix}
Ein $\mathcal{A}$-Kode für die Menge $\mathcal{X}$ mit Alphabet $\mathcal{A}$ ist eine injektive Abbildung (1-zu-1)
$$ \kappa : \mathcal{X} \to \mathcal{W} (\mathcal{A})$$
die jedem Wort $x \in \mathcal{X}$ eindeutig ein Kodewort $\kappa(x)$ zuordnet.\\\\
Ein Wort $v = a_1 \cdots a_j$ heißt Präfix des Wortes $w = b_1 \cdots b_k$, wenn $j \leq k$ und $v = b_1 \cdots b_j$ ist. Das Wort $w$ heisst Fortsetzung von $v$. Ein Kode $$ \kappa : \mathcal{X} \to \mathcal{W} (\mathcal{A})$$
 heißt präfixfrei, wenn kein Kodewort $\kappa(x)$ Präfix eines anderen Kodewortes $\kappa(y)$ ist. 

\subsubsection{Zusammenhang mit der Fragestrategie}
Jede Fragestrategie liefert einen präfixfreien Kode mit Alphabet $\mathcal{A := \{  \text{j} ,\text{n} \}}$.\\
Das Maß für Informationsgehalt (= Entropie) der Quelle $X$ ist das Minimum von $\mathbb{E}(l(\kappa (X)))$ über alle präfixfreien Kodes $\kappa$ für $X$.

\subsubsection{Kraftsche Ungleichung}
Sei $\kappa$ ein präfixfreier $\mathcal{A}$-Kode für $\mathcal{X}$ mit $\# \mathcal{A} = d$. Dann ist
$$ \sum_{x \in \mathcal{X} }  d^{-l( \kappa(x))} \leq 1$$\\\\
Für $x \in \mathcal{X}$ sei $L : \mathcal{X} \to \mathbb{N}$ eine Abbildung, so dass $ \sum_{x \in \mathcal{X} }  d^{-L(x)} \leq 1$ gilt. Dann gibt es einen präfixfreien  $\mathcal{A}$-Kode für $\mathcal{X}$ mit 
$$ l(\kappa(x)) = L(x) \; .$$

\subsubsection{Entropie}
Die Entropie $H_d(X)$ der Ordnung $d$ der Zufallsvariable $X$ ist definiert als das Minimum von $\sum_{x \in \mathcal{X} }  L(x) Q(x)$ über alle Abbildungen $L: \mathcal{X} \to \mathbb{N}$ mit \mbox{$\sum_{x \in \mathcal{X} }  d^{-L(x)} \leq 1$.}\\\\
Die Entropie der Zufallsvariable $X$ ist definiert durch 
$$ H(X) := -\sum_{x \in \mathcal{X} }   Q(x) \log(Q(x))$$\\
Es gilt
$$ \frac{H(X)}{\log(d)} \leq H_d(X) \leq \frac{H(X)}{\log(d)} +1 \; .$$\\\\
Berechne für $x \in \mathcal{X}$ die Funktion $L(x) := \lceil -\log_d(Q(x)) \rceil$. Damit existiert ein präfixfreier $\mathcal{A}$-Kode
$\kappa$ mit $l(\kappa(x)) = L(x)$. Dann gilt $\mathbb{E}(l(\kappa(X))) < H_d(X) +1$.

\subsubsection{Bedeutung}
Eine Fragestrategie ist äquivalent zu einem präfixfreien Kode. Die durchschnittliche Anzahl an notwendigen Fragen zur Bestimmung des Ergebnisses einer Zufallsvariable $X$ ist somit ebenfalls äquivalent zur durchschnittlichen Wortlänge des zugehörigen präfixfreien Kodes.\\
Die Entropie ist ein Maß für den Informationsgehalt einer Zufallsvariable $X$. Je häufiger ein Ergebnis auftritt, desto geringer ist sein Informationswert.\\
Die Entropie beschreibt die durchschnittliche Wortlänge eines optimalen, präfixfreien Kodes für $X$. Dabei ist ein Kode genau dann optimal, wenn die durchschnittliche Wortlänge eines Kodewortes, im Vergleich mit allen möglichen präfixfreien Kodes für $X$, minimal ist.\\
Die minimale durchschnittliche Kodewortlänge (Entropie) hängt auch von den zur Verfügung stehenden Anzahl an Buchstaben $d$ ab (mehr Antwortmöglichkeiten $\rightarrow$ weniger Fragen notwendig).\\
Das die minimale durchschnittliche Kodewortlänge tatsächlich der Entropie $H_d(X)$ entspricht, wird durch die kraftsche Ungleichung impliziert.\\
Die Entropie $H_d(X)$ kann zwar nicht direkt berechnet, aber durch die Entropie-Ungleichung unter Verwendung von $H(X)$ zwischen zwei natürliche Zahlen eingeschätzt werden.

\end{document}
